{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google COLAB Settings\n",
    "In this section are certain processes that should be run when running the code through Google COLAB so to have access to a GPU. If such is the case, uncomment the sections and run them sequentially. otherwise, feel free to skip directly to [Imports](#Imports)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# from os.path import exists\n",
    "# from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "# platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "# cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "# accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "# !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "# !pip install livelossplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive\n",
    "This portion is exclusively for development on _my_ end. I use Google Drive to access the training/testing data without having to redownload it each time the Google COLAB runtime is reset. \n",
    "\n",
    "Of course anyone who does not have access to my Google credentials will not be able to access my Drive. As such, these users should skip directly to [Imports](#Imports). The result will be that torchvision will personally download the CIFAR data from the web each time the COLAB runtime is reset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mounting Drive\n",
    "This mounts Google Drive to the local runtime. If Drive is already mounted, then of course, it will not try to mount it again. It will of course ask for authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing data from Drive\n",
    "Here I import the CIFAR data which I have previously downloaded and stored in my Google Drive. To do so I copy the corresponding directory from my Google Drive into the COLAB Runtime to avoid having to redownload it each time my COLAB runtime is reset. The ```-n``` flag is set to avoid overwriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp -r -n /content/gdrive/My\\ Drive/Education/Undergraduate/Year_3/Computer_Science/SSA/Machine_Learning/Coursework/ML_Classifier-Pegasus-Generator/data /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision  # provides specific datasets\n",
    "import matplotlib.pyplot as plt  # provides plotting capabilities\n",
    "from livelossplot import PlotLosses  # provides live plotting capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets the device for the user.\n",
    "device = torch.device(\n",
    "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Device being used:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(iterable):\n",
    "    \"\"\"Helper function to make getting another batch of data easier\"\"\"\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "            \n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    \"\"\"Plots predicted images\"\"\"\n",
    "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    color = '#335599' if predicted_label == true_label else '#ee4433'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                  100*np.max(predictions_array),\n",
    "                                  class_names[true_label]),\n",
    "                                  color=color)\n",
    "\n",
    "    \n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    \"\"\"Plots the value arrays associated with particular predictions\"\"\"\n",
    "    predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(len(class_names)), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('#ee4433')\n",
    "    thisplot[true_label].set_color('#335599')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''A simple classifier'''\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        # initialize network layers\n",
    "        layers = nn.ModuleList()\n",
    "        # 3 (colors) by 32 (width) by 32 (height) tensor \n",
    "        layers.append(nn.Linear(in_features=3 * 32 * 32, out_features=512))\n",
    "        # rectifier layer\n",
    "        layers.append(nn.ReLU())\n",
    "        # 100 class outputs\n",
    "        layers.append(nn.Linear(in_features=512, out_features=100))\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # flatten input as we're using linear layers\n",
    "        for m in self.layers:\n",
    "            x = m(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define list containing class names\n",
    "class_names = ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'computer_keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle',\n",
    "               'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm', ]\n",
    "\n",
    "# load CIFAR100 training data, shuffling at each epoch, with a batch size of 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])),\n",
    "    shuffle=True, batch_size=16, drop_last=True)\n",
    "\n",
    "# does the same but for testing data, with no shuffling\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.CIFAR100('data', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])),\n",
    "    shuffle=False, batch_size=16, drop_last=True)\n",
    "\n",
    "# create iterators for training and testing\n",
    "train_iterator = iter(cycle(train_loader))\n",
    "test_iterator = iter(cycle(test_loader))\n",
    "\n",
    "print(f'> Size of training dataset {len(train_loader.dataset)}')\n",
    "print(f'> Size of test dataset {len(test_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Some of Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pyplot figure\n",
    "plt.figure(figsize=(10, 10))\n",
    "# make a 5 by 5 grid of images from test dataset\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    # get rid of tickmarks\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    # get rid of grid\n",
    "    plt.grid(False)\n",
    "    # show the image\n",
    "    plt.imshow(test_loader.dataset[i][0].permute(\n",
    "        0, 2, 1).contiguous().permute(2, 1, 0), cmap=plt.cm.binary)\n",
    "    # label\n",
    "    plt.xlabel(class_names[test_loader.dataset[i][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model object\n",
    "N = MyNetwork().to(device)\n",
    "\n",
    "print(f'> Number of network parameters {len(torch.nn.utils.parameters_to_vector(N.parameters()))}')\n",
    "\n",
    "# initialise the optimiser (stochastic gradient descent)\n",
    "optimiser = torch.optim.SGD(N.parameters(), lr=0.001)\n",
    "# set start epoch\n",
    "epoch = 0\n",
    "# initialize live loss plot object\n",
    "liveplot = PlotLosses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while (epoch < 100): # training for 100 epochs\n",
    "\n",
    "    # arrays for metrics\n",
    "    logs = {}\n",
    "    train_loss_arr = np.zeros(0)\n",
    "    train_acc_arr = np.zeros(0)\n",
    "    test_loss_arr = np.zeros(0)\n",
    "    test_acc_arr = np.zeros(0)\n",
    "\n",
    "    # iterate over some of the train dateset\n",
    "    for i in range(1000):\n",
    "        # get input and respective targets from training dataset\n",
    "        x, t = next(train_iterator)\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        \n",
    "        # initialize the gradient to zero\n",
    "        optimiser.zero_grad()\n",
    "        # calculate prediction by running input through Neural Network\n",
    "        p = N(x)\n",
    "        pred = p.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        # calculate loss between prediction and target\n",
    "        loss = torch.nn.functional.cross_entropy(p, t)\n",
    "        # backpropagate loss\n",
    "        loss.backward()\n",
    "        # performs a parameter update\n",
    "        optimiser.step()\n",
    "    \n",
    "        # record the loss for this epoch\n",
    "        train_loss_arr = np.append(train_loss_arr, loss.cpu().data)\n",
    "        # record the accuracy for this epoch\n",
    "        train_acc_arr = np.append(train_acc_arr, pred.data.eq(\n",
    "            t.view_as(pred)).float().mean().item())\n",
    "\n",
    "    # iterate entire test dataset\n",
    "    for x, t in test_loader:\n",
    "        # get input and respective targets from testing dataset\n",
    "        x, t = x.to(device), t.to(device)\n",
    "        \n",
    "        # calculate prediction by running input through Neural Network\n",
    "        p = N(x)\n",
    "        # calculate loss\n",
    "        loss = torch.nn.functional.cross_entropy(p, t)\n",
    "        pred = p.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        # record the loss for this epoch\n",
    "        test_loss_arr = np.append(test_loss_arr, loss.cpu().data)\n",
    "        # record the accuracy for this epoch\n",
    "        test_acc_arr = np.append(test_acc_arr, pred.data.eq(\n",
    "            t.view_as(pred)).float().mean().item())\n",
    "    \n",
    "    # draw the training results live\n",
    "    # NOTE: live plot library has dumb naming forcing our 'test' to be called 'validation'\n",
    "    liveplot.update({\n",
    "        'accuracy': train_acc_arr.mean(),\n",
    "        'val_accuracy': test_acc_arr.mean(),\n",
    "        'loss': train_loss_arr.mean(),\n",
    "        'val_loss': test_loss_arr.mean()\n",
    "    })\n",
    "    liveplot.draw()\n",
    "    \n",
    "    # move on to the next epoch\n",
    "    epoch = epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = next(test_iterator)\n",
    "test_images, test_labels = test_images.to(device), test_labels.to(device)\n",
    "# perform inference on the test dataset - use softmax to format this as a sum of probabilities normalized to 1\n",
    "test_preds = torch.softmax(N(test_images).view(test_images.size(0), len(class_names)), dim=1).data.squeeze().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 4\n",
    "num_cols = 4\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, test_preds, test_labels.cpu(), test_images.cpu().squeeze().permute(1,3,2,0).contiguous().permute(3,2,1,0))\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, test_preds, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
